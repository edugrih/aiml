import kagglehub

# Download latest version
path = kagglehub.dataset_download("lovishbansal123/diamond-dataset")

print("Path to dataset files:", path)

!ls /root/.cache/kagglehub/datasets/lovishbansal123/diamond-dataset/versions/1

import pandas as pd

csv = pd.read_csv(path + "/diamonds.csv")

csv.info()

# Check for null values
csv.isna().sum()

csv.drop_duplicates(inplace=True)

csv.drop('Unnamed: 0', axis=1, inplace=True)

colnames = { 'x': 'length', 'y': 'width', 'z': 'depth', 'depth': 'total_depth'}

csv.rename(columns = colnames, inplace=True)

csv.info()

csv.describe()

display(pd.DataFrame(csv['cut'].value_counts()))

import matplotlib.pyplot as plt
import seaborn as sns


plt.figure(figsize=(10,8))
sns.countplot(x = csv['cut'], hue= csv['color'])
plt.title('Number of diamonds based on cut')
plt.xlabel('Cut')
plt.show()

display(pd.DataFrame(csv.groupby('cut')['price'].mean()))

plt.figure(figsize=(10, 8))
sns.barplot(x=csv['cut'], y=csv['price'])
plt.title('Average price of Diamond based on cut')
plt.show()

# Stats models API for statistical APIs
import statsmodels.api as sm
# Import Ordinary Least Square (OLS) module
from statsmodels.formula.api import ols

# For running some stats on data
import scipy.stats as stats

model = ols('price ~ color + cut + color:cut', data = csv).fit()
anova_table = sm.stats.anova_lm(model, typ = 2)
print(anova_table)

csv['area'] = csv['length']*csv['width']
csv.info()

# Checking the correlation between the features
correlation_matrix = csv[['carat', 'total_depth', 'table', 'price', 'length', 'width', 'depth', 'area']].corr()

# Plot heatmap based on correlation-matrix
plt.figure(figsize = (10, 8))
sns.heatmap(correlation_matrix, annot = True, cmap = 'coolwarm', fmt = '.2f', linewidth = 0.5)
plt.title('Correlation heatmap')
plt.show()

## **Now lets build the prediction models using different algorithms**

To start lets first build a Linear Regression Model

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error, r2_score


# Dependent or result variable
Y = csv['price'].to_numpy(float)

# Independent variables which factor in for the dependent variable
X = csv[['carat','depth', 'color', 'cut', 'clarity', 'area', 'length', 'width']]
X = pd.get_dummies(X).to_numpy(dtype = float)

# Split into training and test data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.3, random_state=1)

scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

#instantiate the model
lr = LinearRegression()

lr = lr.fit(X_train, Y_train)
print('Trainning accuracy:', r2_score(Y_train, lr.predict(X_train)))

import numpy as np

y_pred = lr.predict(X_test)
print('Coefficient of determination :', r2_score(Y_test, y_pred))
print('Root mean squared error:', np.sqrt(mean_squared_error(Y_test, y_pred)))

Now lets look at Ridge regression

ridge = Ridge()
parameters = {'alpha': [0.01, 0.1, 1.0, 10.0]}
grid_search = GridSearchCV(estimator = ridge, param_grid = parameters, cv = 5)
grid_search.fit(X_train, Y_train)
print('The best Alpha value:', grid_search.best_params_)

ridge = Ridge(alpha = 10)
ridge.fit(X_train, Y_train)
print('Coefficient of determination :', r2_score(Y_test, ridge.predict(X_test)))
print('Root mean squared error:', np.sqrt(mean_squared_error(Y_test, ridge.predict(X_test))))

# **Lets build a deep learning model now**

import tensorflow as tf
from tensorflow import keras
from keras import layers


output_size=1
hidden_layer=3
input_size=1
learning_rate=0.01
loss_function='mean_squared_error'
epochs=50
batch_size=10

model = keras.Sequential()
model.add(keras.layers.Dense(hidden_layer, activation = 'relu'))
model.add(keras.layers.Dense(output_size))
model.compile(keras.optimizers.Adam(learning_rate = learning_rate), loss_function)

history = model.fit(X_train, Y_train, epochs = epochs, batch_size = batch_size,
                    verbose = False, validation_split = 0.3)

def plot_loss(history):
    plt.plot(history.history['loss'], label='loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.legend()
    plt.grid(True)

plot_loss(history)

y_pred = model.predict(X_test)
print('Coefficient of determination: ', r2_score(Y_test, y_pred))
print('RMSE:', np.sqrt(mean_squared_error(Y_test, y_pred)))
